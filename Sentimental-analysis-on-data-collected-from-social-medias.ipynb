{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Projet-2:-Social-Network-Analysis\">Social Network Analysis<a class=\"anchor-link\" href=\"#Projet-2:-Social-Network-Analysis\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Deuxième-Partie\">Partie Twitter<a class=\"anchor-link\" href=\"#Deuxième-Partie\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"On-va-tout-d'abord-télécharger-la-librairie-Tweepy-qui-sert-à-travailler-avec-Twitter.\">On va tout d'abord télécharger la librairie Tweepy qui sert à travailler avec Twitter.<a class=\"anchor-link\" href=\"#On-va-tout-d'abord-télécharger-la-librairie-Tweepy-qui-sert-à-travailler-avec-Twitter.\">¶</a></h4><h5 id=\"On-utilise-la-commande-suivante-dans-la-console:\">On utilise la commande suivante dans la console:<a class=\"anchor-link\" href=\"#On-utilise-la-commande-suivante-dans-la-console:\">¶</a></h5><p>pip install tweepy</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tweepy\n",
    " \n",
    "auth = tweepy.OAuthHandler(\"XXXXXXXXXXXX\", \"XXXXXXXXXXXXXXXXXXXX\")\n",
    "auth.set_access_token(\"XXXXXXXXXXXXXX\", \"XXXXXXXXXXXXXXXXXXXXXXX \")\n",
    " \n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = tweepy.Cursor(api.search,q = \"Trump\", l=\"en\").items(400)\n",
    "idTweet=[]\n",
    "screenName=[]\n",
    "text=[]\n",
    "createdAt=[]\n",
    "for r in results:\n",
    "    idTweet.append(r.id)\n",
    "    screenName.append(r.user.screen_name)\n",
    "    text.append(r.text)\n",
    "    createdAt.append(r.created_at)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=pd.DataFrame()\n",
    "data['idTweet']=idTweet\n",
    "data['screenName']=screenName\n",
    "data['text']=text\n",
    "data['createdAt']=createdAt\n",
    "data['createdAt']=pd.to_datetime(data['createdAt'])\n",
    "data['EST'] = data['createdAt'] - pd.Timedelta(hours=5)\n",
    "data['EST'] = pd.to_datetime(data['EST'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allUsers=list(data['screenName'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.to_csv('results.csv',index = False,encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-995e729ca5b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdataUsers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataUsers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallUsers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "dataUsers=pd.DataFrame()\n",
    "dataUsers['userFromName']=[]\n",
    "dataUsers['userFromId']=[]\n",
    "dataUsers['userToId']=[]\n",
    "count=0\n",
    "for name in allUsers:\n",
    "    followers = []\n",
    "    for page in tweepy.Cursor(api.followers_ids, screen_name=name).pages():\n",
    "        followers.extend(page)\n",
    "    \n",
    "    currentId = api.get_user(screen_name=name).id\n",
    "    currentId = [currentId] * len(followers)\n",
    "    currentName = [name] * len(followers)   \n",
    "    \n",
    "    dataT = pd.DataFrame()\n",
    "    dataT['userFromName'] = currentName\n",
    "    dataT['userFromId'] = currentId\n",
    "    dataT['userToId'] = followers\n",
    "    \n",
    "    dataUsers = pd.concat([dataUsers,dataT])\n",
    "    time.sleep(70) \n",
    "    count +=1\n",
    "    per = round(count*100.0/len(allUsers),1)\n",
    "    sys.stdout.write(\"\\rTwitter call %s%% complete.\" % per)\n",
    "    sys.stdout.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fromId = dataUsers['userFromId'].unique()\n",
    "dataChat = dataUsers[dataUsers['userToId'].apply(lambda x: x in fromId)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataLookup = dataChat[['userFromName','userFromId']]\n",
    "dataLookup = dataLookup.drop_duplicates()\n",
    "dataLookup.columns = ['userToName','userToId']\n",
    "dataCommunity = dataUsers.merge(dataLookup, on='userToId')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataCommunity.to_csv('dataCommunity.csv',index = False,encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=pd.read_csv('dataCommunity.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "import networkx as nx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.userFromId=data.userFromId.apply(lambda x: int(x))\n",
    "data.userToId=data.userToId.apply(lambda x:int(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "?zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Graph = nx.DiGraph()\n",
    "Graph.add_nodes_from(data['userFromId'])\n",
    "Graph.add_edges_from(zip(data['userFromId'],data['userToId']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lu=data[['userFromName','userFromId']].drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for uid in lu['userFromId']:\n",
    "    Graph.node[uid]['userName'] = lu['userFromName'][data['userFromId']==uid].values[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nx.draw(Graph, pos=nx.spring_layout(Graph,k=.1600),node_color='blue',edge_color='black')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "polarity=0.0\n",
    "i=0\n",
    "pn=0\n",
    "pz=0\n",
    "pp=0\n",
    "for t in text:\n",
    "    blob = TextBlob(t)\n",
    "    for sentence in blob.sentences:\n",
    "        i=i+1\n",
    "        polarity=polarity+sentence.sentiment.polarity\n",
    "        if sentence.sentiment.polarity<0:\n",
    "            print(\"\\033[1m\"+\"negative sentiment\"+\"\\033[0;0m\")\n",
    "            print(sentence)\n",
    "            print(sentence.sentiment.polarity)\n",
    "            pn=pn+1\n",
    "        if sentence.sentiment.polarity>0:\n",
    "            print(\"\\033[1m\"+\"positive sentiment\"+\"\\033[0;0m\")\n",
    "            print(sentence)\n",
    "            print(sentence.sentiment.polarity)\n",
    "            pp=pp+1\n",
    "        if sentence.sentiment.polarity==0:\n",
    "            print(\"\\033[1m\"+\"neutral sentiment\"+\"\\033[0;0m\")\n",
    "            print(sentence)\n",
    "            print(sentence.sentiment.polarity)\n",
    "            pz=pz+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "globalPol=polarity/i\n",
    "print(\"\\033[1m Global polority:\\033[0;0m \",globalPol)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\033[1m Neutral sentiment:\\033[0;0m  \",100*pn/i,\" %\")\n",
    "print(\"\\033[1m Positive sentiment:\\033[0;0m \",100*pp/i,\"%\")\n",
    "print(\"\\033[1m Negative sentiment:\\033[0;0m \",100*pz/i,\" %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Partie-YouTube\">Partie YouTube<a class=\"anchor-link\" href=\"#Partie-YouTube\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Importation-des-librairies\">Importation des librairies<a class=\"anchor-link\" href=\"#Importation-des-librairies\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pytube\n",
    "import time\n",
    "from selenium.webdriver import Chrome\n",
    "from contextlib import closing\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from subprocess import check_output\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Récupération-des-liens\">Récupération des liens<a class=\"anchor-link\" href=\"#Récupération-des-liens\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Nous allons récupérer les liens disponibles sur Youtube en relation avec la phrase clé \"Donald Trump\"</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base = \"https://www.youtube.com/results?search_query=\"\n",
    "qstring = \"donald trump\"\n",
    "r = requests.get(base+qstring)\n",
    "page = r.text\n",
    "soup=bs(page,'html.parser')\n",
    "vids = soup.findAll('a',attrs={'class':'yt-uix-tile-link'})\n",
    "videolist=[]\n",
    "for v in vids:\n",
    "    tmp = 'https://www.youtube.com' + v['href']\n",
    "    videolist.append(tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Visualisation des liens</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for item in videolist:\n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Récuperation-des-commentaires\">Récuperation des commentaires<a class=\"anchor-link\" href=\"#Récuperation-des-commentaires\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")\n",
    "ListComs=[]\n",
    "ListDesId=[]\n",
    "for i in range(len(ListComs)):\n",
    "    ListDesId.append(i) \n",
    "with closing(Chrome(chrome_options=chrome_options)) as driver:\n",
    "    \n",
    "    for i in range(len(videolist)):\n",
    "        \n",
    "        wait = WebDriverWait(driver,10)\n",
    "        MyList.append(videolist[i])\n",
    "        driver.get(videolist[i])\n",
    "        for item in range(3):\n",
    "            wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"body\"))).send_keys(Keys.END)\n",
    "            time.sleep(3)\n",
    "\n",
    "        for comment in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#comment #content-text\"))):\n",
    "            ListComs.append(comment.text)\n",
    "            print(comment.text)\n",
    "            \n",
    "       \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><b>Les commentaires sont stockés dans un fichier CSV</b></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=pd.DataFrame()\n",
    "data['Id']=ListDesId\n",
    "data['comments']=ListComs\n",
    "data.to_csv('Youtube.csv',index = False,encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Analyse-Sentimentale\">Analyse Sentimentale<a class=\"anchor-link\" href=\"#Analyse-Sentimentale\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Dans cette étape on va voir quels sont les commentaires positifs et quels sont les commentaires négatifs</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "polarity=0.0\n",
    "i=0\n",
    "neutral=0\n",
    "positive=0\n",
    "negative=0\n",
    "listPos=[]\n",
    "listNeg=[]\n",
    "PosPolarity=0.0\n",
    "NegPolarity=0.0\n",
    "with open('Youtube.csv', encoding=\"utf8\") as f:\n",
    "    for s in f:\n",
    "        blob = TextBlob(s)\n",
    "        for sentence in blob.sentences:\n",
    "            i=i+1\n",
    "            polarity=polarity+sentence.sentiment.polarity\n",
    "            if sentence.sentiment.polarity<0:\n",
    "                print(\"\\033[1m\"+\"negative sentiment\"+\"\\033[0;0m\")\n",
    "                print(sentence)\n",
    "                print(sentence.sentiment.polarity)\n",
    "                negative=negative+1\n",
    "                NegPolarity=NegPolarity+sentence.sentiment.polarity\n",
    "                listNeg.append(sentence)\n",
    "            if sentence.sentiment.polarity>0:\n",
    "                print(\"\\033[1m\"+\"positive sentiment\"+\"\\033[0;0m\")\n",
    "                print(sentence)\n",
    "                print(sentence.sentiment.polarity)\n",
    "                positive=positive+1\n",
    "                PosPolarity=PosPolarity+sentence.sentiment.polarity\n",
    "                listPos.append(sentence)\n",
    "            if sentence.sentiment.polarity==0:\n",
    "                print(\"\\033[1m\"+\"neutral sentiment\"+\"\\033[0;0m\")\n",
    "                print(sentence)\n",
    "                print(sentence.sentiment.polarity)\n",
    "                neutral=neutral+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><b>Séparer les commentaires positifs et négatifs dans 2 fichiers CSV diffèrents</b></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datapos=pd.DataFrame()\n",
    "datapos['comments']=listPos\n",
    "datapos.to_csv('YoutubePositive.csv',index = False,encoding='utf-8')\n",
    "dataneg=pd.DataFrame()\n",
    "dataneg['comments']=listNeg\n",
    "dataneg.to_csv('YoutubeNegative.csv',index = False,encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MydataPos=pd.read_csv('YoutubePositive.csv')\n",
    "MydataNeg=pd.read_csv('YoutubeNegative.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Traitement-automatique-du-langage-naturel\">Traitement automatique du langage naturel<a class=\"anchor-link\" href=\"#Traitement-automatique-du-langage-naturel\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Il s'agit de visualiser les mots qui reviennent dans les commentaires et qui caractérisent une réactions positive ou négative au contenu de la vidéo</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wordcloud_draw(data, color = 'black'):\n",
    "    words = ' '.join(data)\n",
    "    cleaned_word = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and not word.startswith('#')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color=color,\n",
    "                      width=2500,\n",
    "                      height=2000\n",
    "                     ).generate(cleaned_word)\n",
    "    plt.figure(1,figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Positive words\")\n",
    "wordcloud_draw(MydataPos['comments'],'white')\n",
    "print(\"Negative words\")\n",
    "wordcloud_draw(MydataNeg['comments'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Récupération-des-descriptions-des-vidéos\">Récupération des descriptions des vidéos<a class=\"anchor-link\" href=\"#Récupération-des-descriptions-des-vidéos\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "desc=[]\n",
    "for item in videolist:\n",
    "    s = requests.Session()\n",
    "    r = s.get(item)\n",
    "    html = r.text\n",
    "    soup = bs(html, 'lxml')\n",
    "    for i in soup.find_all('p', id='eow-description'):\n",
    "        for br in i.find_all('br'):\n",
    "            next_sib = br.next_sibling\n",
    "            desc.append(next_sib)\n",
    "desc            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Stockage des descriptions dans un fichier CSV</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataDesc=pd.DataFrame()\n",
    "dataDesc['text']=desc\n",
    "dataDesc.to_csv('Descriptions.csv',index = False,encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Calcul des polarités(positifs/négatifs) des descriptions des vidéos</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "polarity=0.0\n",
    "i=0\n",
    "neutral=0\n",
    "positive2=0\n",
    "negative2=0\n",
    "PosPolarity2=0.0\n",
    "NegPolarity2=0.0\n",
    "with open('Descriptions.csv', encoding=\"utf8\") as f:\n",
    "    for s in f:\n",
    "        blob = TextBlob(s)\n",
    "        for sentence in blob.sentences:\n",
    "            i=i+1\n",
    "            polarity=polarity+sentence.sentiment.polarity\n",
    "            if sentence.sentiment.polarity<0:\n",
    "               \n",
    "                negative2=negative2+1\n",
    "                NegPolarity2=NegPolarity2+sentence.sentiment.polarity\n",
    "                \n",
    "            if sentence.sentiment.polarity>0:\n",
    "                \n",
    "                positive2=positive2+1\n",
    "                PosPolarity2=PosPolarity2+sentence.sentiment.polarity\n",
    "                \n",
    "            if sentence.sentiment.polarity==0:\n",
    "               \n",
    "                neutral=neutral+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Comparaison-des-polarités-entre-les-commentaires-et-les-descriptions-des-vidéos\">Comparaison des polarités entre les commentaires et les descriptions des vidéos<a class=\"anchor-link\" href=\"#Comparaison-des-polarités-entre-les-commentaires-et-les-descriptions-des-vidéos\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Moyenne de la polarité positive des commentaires</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MoyPosPolarity=PosPolarity/positive\n",
    "MoyPosPolarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Moyenne de la polarité négative des commentaires</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MoyNegPolarity=NegPolarity/negative\n",
    "MoyNegPolarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Moyenne de la polarité positive des descriptions</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MoyPosPolarity2=PosPolarity2/positive2\n",
    "MoyPosPolarity2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Moyenne de la polarité négative des descriptions</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MoyNegPolarity2=NegPolarity2/negative2\n",
    "MoyNegPolarity2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"Interprétations\">Interprétations<a class=\"anchor-link\" href=\"#Interprétations\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>On remarque que la moyenne de la polartité positive dans les descriptions est largement supérieure à celle des commentaires.\n",
    "On remarque que la moyenne de la polartité négative dans les descriptions est largement supérieure à celle des commentaires.\n",
    "On peut donc dire que les commentaires et les descriptions ne sont pas proportionnelles et vu que les moyennes des descriptions sont élevés on peut émettre une hypothèse sur le fait que les descriptions des vidéos sont exagérées dans le bon ou le mauvais sens</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Partie Facebook</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Importation des librairies:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Préparation du webdriver:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chrome_path = r\"D:\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(chrome_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Connexion au compte facebook:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver.get(\"https://www.facebook.com\")\n",
    "driver.find_element_by_name(\"email\").clear()\n",
    "driver.find_element_by_name(\"pass\").clear()\n",
    "driver.find_element_by_name(\"email\").send_keys(\"insert email here\")\n",
    "driver.find_element_by_name(\"pass\").send_keys(\"insert pwd here\")\n",
    "driver.find_element_by_id(\"loginbutton\").click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Récupération des données:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver.get(\"https://www.facebook.com/DonaldTrump/photos/a.10156483516640725/10161764020965725/?type=3&__xts__%5B0%5D=68.ARCA05t9NUHm4QghbZBo55ni01UGzZkS62ao_eTuKzsfiHrYuRBESO5TTmlbdO5puQinluVw6qQEHesP-RDgLffbM-yJvZEMISHIfnzZ3rGGmtmIrjPud_uNMF_tI7DAg8TqXBYHE19h3jOUzgAazOk4KVkX15Lwfm7kosPRjkMRB30Qg9eeBbJT4iGbZGtWXQVknUodgS61lIimbHBlrkt-UwncLmC5u7SPKW1peuHUTGB0gpVIQJYsdaY_RLU5w3uEQeaHz5sNSS-qzPIYG0aBm4p-dhNDhbBxHVmVo-IDCFPEs3lhbauBCBEd6o4dGaoSGqcOv8lexKGQ5sA9&__tn__=-R\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,10):\n",
    "    driver.find_elements_by_xpath(\"//*[contains(text(), 'Voir plus de commentaires')]\")[-1].click()\n",
    "    time.sleep(2)\n",
    "posts = driver.find_elements_by_class_name(\"UFICommentBody\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Ecrtiture des données dans un fichier texte:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for post in posts:\n",
    "    with open(\"trump.txt\", \"a\", encoding='utf-8') as document:\n",
    "        document.write(post.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Lecture des données du fichier texte:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts=[]\n",
    "with open('trump.txt', 'r') as f:\n",
    "    for s in f:\n",
    "        texts.append(s)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Analyse Sentimentale des données avec textblob et nltk:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "polarity=0.0\n",
    "i=0\n",
    "neutral=0\n",
    "positive=0\n",
    "negative=0\n",
    "text=[]\n",
    "sen=[]\n",
    "for s in texts:\n",
    "    blob = TextBlob(s)\n",
    "    for sentence in blob.sentences:\n",
    "        i=i+1\n",
    "        polarity=polarity+sentence.sentiment.polarity\n",
    "        if sentence.sentiment.polarity<0:\n",
    "            print(\"\\033[1m\"+\"negative sentiment\"+\"\\033[0;0m\")\n",
    "            print(sentence)\n",
    "            print(sentence.sentiment.polarity)\n",
    "            text.append(sentence)\n",
    "            sen.append('negative')\n",
    "            negative=negative+1\n",
    "        if sentence.sentiment.polarity>0:\n",
    "            print(\"\\033[1m\"+\"positive sentiment\"+\"\\033[0;0m\")\n",
    "            print(sentence)\n",
    "            print(sentence.sentiment.polarity)\n",
    "            text.append(sentence)\n",
    "            sen.append('positive')\n",
    "            positive=positive+1\n",
    "        if sentence.sentiment.polarity==0:\n",
    "            print(\"\\033[1m\"+\"neutral sentiment\"+\"\\033[0;0m\")\n",
    "            print(sentence)\n",
    "            print(sentence.sentiment.polarity)\n",
    "            neutral=neutral+1\n",
    "            text.append(sentence)\n",
    "            sen.append('neutral')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d=[]\n",
    "d.append(negative)\n",
    "d.append(positive)\n",
    "d.append(neutral)\n",
    "d\n",
    "df = pd.DataFrame(data=d,index=['Negative','Positive','Neutral'],columns=['count'])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Exportation de l'analyse vers un fichier csv:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataF=pd.DataFrame(data={'text':text,'sentiment':sen\n",
    "                         })\n",
    "dataF\n",
    "dataF.to_csv('facebookSentiments.csv',index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Lecture du fichier csv:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fs=pd.read_csv('facebookSentiments.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Préparation des données:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = fs[['text','sentiment']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into train and test set\n",
    "train, test = train_test_split(data,test_size = 0.1)\n",
    "# Removing neutral sentiments\n",
    "train = train[train.sentiment != \"neutral\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>On a éliminé les sentiments de type \"neutral\"</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Dans cette partie , on va visualiser les mots positifs et les mots négatifs</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_pos = train[ train['sentiment'] == 'positive']\n",
    "train_pos = train_pos['text']\n",
    "train_neg = train[ train['sentiment'] == 'negative']\n",
    "train_neg = train_neg['text']\n",
    "\n",
    "def wordcloud_draw(data, color = 'black'):\n",
    "    words = ' '.join(data)\n",
    "    cleaned_word = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and not word.startswith('#')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color=color,\n",
    "                      width=2500,\n",
    "                      height=2000\n",
    "                     ).generate(cleaned_word)\n",
    "    plt.figure(1,figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Positive words\")\n",
    "wordcloud_draw(train_pos,'white')\n",
    "print(\"Negative words\")\n",
    "wordcloud_draw(train_neg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Stop Word: \n",
    "Les mots \"stop word\" sont des mots qui ne contiennent pas de signification importante quand à leur utilisation dans les requêtes de recherche. Habituellement, ces mots sont filtrés des requêtes de recherche car ils renvoient une grande quantité d'informations inutiles (le, pour, etc...).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fbcomments = []\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n",
    "    words_cleaned = [word for word in words_filtered\n",
    "        if 'http' not in word\n",
    "        and not word.startswith('@')\n",
    "        and not word.startswith('#')]\n",
    "    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n",
    "    fbcomments.append((words_without_stopwords, row.sentiment))\n",
    "\n",
    "test_pos = test[ test['sentiment'] == 'positive']\n",
    "test_pos = test_pos['text']\n",
    "test_neg = test[ test['sentiment'] == 'negative']\n",
    "test_neg = test_neg['text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting word features\n",
    "def get_words_in_fbcomments(fbcomments):\n",
    "    all = []\n",
    "    for (words, sentiment) in fbcomments:\n",
    "        all.extend(words)\n",
    "    return all\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    features = wordlist.keys()\n",
    "    return features\n",
    "w_features = get_word_features(get_words_in_fbcomments(fbcomments))\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in w_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>On a extrait les caractéristiques avec nltk lib, tout d'abord en mesurant une distribution fréquente et en sélectionnant les mots résultants.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordcloud_draw(w_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>On a donc tracé les mots les plus fréquemment distribués. La plupart des mots sont centrés autour de \"thank\".</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training the Naive Bayes classifier\n",
    "training_set = nltk.classify.apply_features(extract_features,fbcomments)\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>À l’aide du classifieur Naive Bayle nltk, on a classé les caractéristiques des mots extraits.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neg_cnt = 0\n",
    "pos_cnt = 0\n",
    "for obj in test_neg: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'negative'): \n",
    "        neg_cnt = neg_cnt + 1\n",
    "for obj in test_pos: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'positive'): \n",
    "        pos_cnt = pos_cnt + 1\n",
    "        \n",
    "print('[Negative]: %s/%s '  % (len(test_neg),neg_cnt))        \n",
    "print('[Positive]: %s/%s '  % (len(test_pos),pos_cnt))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>=&gt; Score de l'algorithme du classificateur</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Interprétation:\">Interprétation:<a class=\"anchor-link\" href=\"#Interprétation:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>D'après l'analyse sentimentale , on remarque que la plus part des commentaires reflètent un sentiment neutre de la part des interlocuteurs. Par contre, où on peut faire une interprétation logique c'est en regardant les commentaires renvoyant un sentiment positif ou négatif. En effet, 203 négatifs contre 337 positifs qui peut nous dire que les commentaires virent vers l'approbation des avis et des paroles cités par \"Donald Trump\".</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Dans une deuxième partie, on essayé de visualiser les mots les plus utilisés par les internautes et on a fait une classification qui les séparent entre des mots négatifs et des mots positifs, qui peut certainement aider à caractériser les messages et les commentaires destinés à \"Donald Trump\".</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
